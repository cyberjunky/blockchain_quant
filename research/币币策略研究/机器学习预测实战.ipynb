{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "tf.random.set_seed(10)\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import talib\n",
    "# pd.set_option('display.max_columns',None)\n",
    "# pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_klinedata(platform, symbol, granularity):\n",
    "    if platform == \"huobi\":  # 2000条数据，时间粒度1min, 5min, 15min, 30min, 60min, 4hour, 1day, 1mon, 1week, 1year\n",
    "        huobi_granularity_dict = {60: \"1min\", 300: \"5min\", 900: \"15min\", 1800: \"30min\", 3600: \"60min\",\n",
    "                                  14400: \"4hour\", 86400: \"1day\", 604800: \"1week\", 2592000: \"mon\",\n",
    "                                  946080000: \"1year\"}\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                res = requests.get(\"https://api.huobi.pro/market/history/kline?period={}&size=2000&symbol={}\".format(\n",
    "                    huobi_granularity_dict[granularity], symbol.replace(\"_\", \"\")), timeout=1)\n",
    "                data = json.loads(res.content.decode())[\"data\"]\n",
    "                df=pd.DataFrame()\n",
    "                if res.status_code==200:\n",
    "                    data = json.loads(res.content.decode())[\"data\"][::-1]\n",
    "                    df['close'] = [i['close'] for i in data]\n",
    "                    df['high'] =[i['high'] for i in data]\n",
    "                    df['low'] =[i['low'] for i in data]\n",
    "                    df['open'] = [i['open'] for i in data]\n",
    "                    df['volume']= [i['vol'] for i in data]\n",
    "                    df['time'] = [time.strftime(\"%Y-%m-%d\", time.localtime(i['id'])) for i in data]\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                df = pd.DataFrame()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据去极值及标准化\n",
    "def winsorize_and_standarlize(data,qrange=[0.05,0.95],axis=0):\n",
    "    '''\n",
    "    input:\n",
    "    data:Dataframe or series,输入数据\n",
    "    qrange:list,list[0]下分位数，list[1]，上分位数，极值用分位数代替\n",
    "    '''\n",
    "    if isinstance(data,pd.DataFrame):\n",
    "        if axis == 0:\n",
    "            q_down = data.quantile(qrange[0])\n",
    "            q_up = data.quantile(qrange[1])\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q_up[n]] = q_up[n]\n",
    "                data[n][data[n] < q_down[n]] = q_down[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.fillna(0)\n",
    "        else:\n",
    "            data = data.stack()\n",
    "            data = data.unstack(0)\n",
    "            q = data.quantile(qrange)\n",
    "            index = data.index\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q[n]] = q[n]\n",
    "            data = (data - data.mean())/data.std()\n",
    "            data = data.stack().unstack(0)\n",
    "            data = data.fillna(0)\n",
    "            \n",
    "    elif isinstance(data,pd.Series):\n",
    "        name = data.name\n",
    "        q = data.quantile(qrange)\n",
    "        data[data>q] = q\n",
    "        data = (data - data.mean())/data.std()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor(symbol):\n",
    "    df = get_klinedata(\"huobi\", symbol, granularity=86400)\n",
    "    df['symbol'] = symbol\n",
    "    df['MOM_10'] = df['close'] / df['close'].shift(9) - 1\n",
    "    df['MOM_60'] = df['close'] / df['close'].shift(59) - 1\n",
    "    df['MOM_120'] = df['close'] / df['close'].shift(119) - 1\n",
    "    df['VSTD_20'] = df['volume'].rolling(20).std()\n",
    "    df['std_20'] = df['close'].rolling(20).std()\n",
    "    df['std_40'] = df['close'].rolling(40).std()\n",
    "    df['std_120'] = df['close'].rolling(120).std()\n",
    "    df['MA_5'] = df['close'].rolling(5).mean()\n",
    "    df['MA_20'] = df['close'].rolling(10).mean()\n",
    "    df['MA_Cross'] = np.where(df['MA_5'] > df['MA_20'], 1, 0)\n",
    "    df['label'] = np.where(df['close'] > df['close'].shift(-7), 0, 1)\n",
    "    # 复合动量因子\n",
    "    df['ret'] = np.log(df['close'] / df['close'].shift())\n",
    "    df['ret_60'] = df['ret'].rolling(60).sum()\n",
    "    # 收益率偏度因子\n",
    "    df['skew_60'] = df['ret'].rolling(60).skew()\n",
    "    df['MACD'] = talib.MACD(df.close, fastperiod=6, slowperiod=12, signalperiod=9)[-1]\n",
    "    df['RSI'] = talib.RSI(df.close, timeperiod=12)\n",
    "    df['MOM'] = talib.MOM(df.close, timeperiod=5)\n",
    "    df['CCI'] = talib.CCI(df.high, df.low, df.close, timeperiod=14)\n",
    "    df['WILLR'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    df['SAR'] = talib.SAR(df.high, df.low)\n",
    "    df['OBV']=talib.OBV(df.close,df.volume)\n",
    "    df['ADOSC'] = talib.ADOSC(df.high, df.low, df.close,df.volume)\n",
    "    df['ROC']=talib.ROC(df.volume) #新增1\n",
    "    df['SLOWK'],df['SLOWD']=talib.STOCH(df.high, df.low, df.close)\n",
    "    df['HT']=talib.HT_TRENDLINE(df.close)\n",
    "    df['ADX']=talib.ADX(df.high,df.low,df.close)\n",
    "    df['APO']=talib.APO(df.close)\n",
    "    df['AROONDOWN'],df['AROONUP']=talib.AROON(df.high,df.low)\n",
    "    df['AROONOSC']=talib.AROONOSC(df.high,df.low)\n",
    "    df['BOP']=talib.BOP(df.open,df.high,df.low,df.close)\n",
    "    df['CMO']=talib.CMO(df.close)\n",
    "    df['MFI']=talib.MFI(df.high,df.low,df.close,df.volume)\n",
    "    df['PPO']=talib.PPO(df.close)\n",
    "    df['ULTOSC']=talib.ULTOSC(df.high,df.low,df.close) #新增2\n",
    "    df['AD']=talib.AD(df.high,df.low,df.close,df.volume)\n",
    "    df['COL3BLACKCROWS']=talib.CDL3BLACKCROWS(df.open, df.high, df.low, df.close)\n",
    "    df['CDLDOJI']=talib.CDLDOJI(df.open,df.high,df.low,df.close)\n",
    "    df['CDLENGULFING']=talib.CDLENGULFING(df.open, df.high, df.low, df.close)\n",
    "    df['CDLHAMMER']=talib.CDLHAMMER(df.open, df.high, df.low, df.close)\n",
    "    df['CDLMORNINGSTAR']=talib.CDLMORNINGSTAR(df.open, df.high, df.low, df.close)\n",
    "    df['TSF']=talib.TSF(df.close)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbols_factor(symbols):\n",
    "    data= pd.DataFrame()\n",
    "    for symbol in symbols:\n",
    "        symbol = \"{}_usdt\".format(symbol)\n",
    "        df = get_factor(symbol)\n",
    "        data= pd.concat([data, df], axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "huobifuture_api_url='https://api.btcgateway.pro'\n",
    "symbols = ['btc', 'eth', 'link', 'eos', 'fil', 'ltc', 'dot', 'doge','bsv','ada','etc','trx','xmr']\n",
    "df_sum=get_symbols_factor(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum=df_sum.sample(frac=1.0,random_state=1)#打乱所有数据\n",
    "df_sum=df_sum.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=int(len(df_sum)*0.7)\n",
    "train=df_sum.iloc[:split]\n",
    "test=df_sum.iloc[split:]\n",
    "features =df_sum.columns.difference(['close','high','low','open','volume','time','label','ret','MA_5','MA_20','symbol',\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=35)\n",
    "x_train=np.array(winsorize_and_standarlize(train[features]))\n",
    "y_train=np.array(train.label)\n",
    "x_test=np.array(winsorize_and_standarlize(test[features]))\n",
    "y_test=np.array(test.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1).reshape(x_train.shape[0], -1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1).reshape(x_test.shape[0], -1)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())   #Flatten the images! Could be done with numpy reshape\n",
    "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu, input_shape= x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(64,activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.sigmoid))   #2 because dataset is numbers from 0 - 1\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),  # Good default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
    "              metrics=['accuracy'],\n",
    "              )  # what to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5284 - val_loss: 0.6919 - val_accuracy: 0.5213\n",
      "Epoch 2/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5327 - val_loss: 0.6912 - val_accuracy: 0.5213\n",
      "Epoch 3/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5365 - val_loss: 0.6908 - val_accuracy: 0.5284\n",
      "Epoch 4/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5442 - val_loss: 0.6903 - val_accuracy: 0.5355\n",
      "Epoch 5/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5511 - val_loss: 0.6893 - val_accuracy: 0.5458\n",
      "Epoch 6/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.5616 - val_loss: 0.6878 - val_accuracy: 0.5624\n",
      "Epoch 7/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.5626 - val_loss: 0.6868 - val_accuracy: 0.5600\n",
      "Epoch 8/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.5699 - val_loss: 0.6834 - val_accuracy: 0.5742\n",
      "Epoch 9/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5816 - val_loss: 0.6812 - val_accuracy: 0.5790\n",
      "Epoch 10/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5853 - val_loss: 0.6812 - val_accuracy: 0.5687\n",
      "Epoch 11/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5872 - val_loss: 0.6760 - val_accuracy: 0.5924\n",
      "Epoch 12/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5960 - val_loss: 0.6770 - val_accuracy: 0.5932\n",
      "Epoch 13/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.5986 - val_loss: 0.6735 - val_accuracy: 0.5885\n",
      "Epoch 14/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6635 - accuracy: 0.5973 - val_loss: 0.6692 - val_accuracy: 0.6019\n",
      "Epoch 15/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6599 - accuracy: 0.6059 - val_loss: 0.6674 - val_accuracy: 0.5893\n",
      "Epoch 16/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6079 - val_loss: 0.6663 - val_accuracy: 0.5987\n",
      "Epoch 17/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.6120 - val_loss: 0.6644 - val_accuracy: 0.6011\n",
      "Epoch 18/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6123 - val_loss: 0.6614 - val_accuracy: 0.6051\n",
      "Epoch 19/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6136 - val_loss: 0.6601 - val_accuracy: 0.6145\n",
      "Epoch 20/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6220 - val_loss: 0.6583 - val_accuracy: 0.6058\n",
      "Epoch 21/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6179 - val_loss: 0.6558 - val_accuracy: 0.6090\n",
      "Epoch 22/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6215 - val_loss: 0.6542 - val_accuracy: 0.6106\n",
      "Epoch 23/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6220 - val_loss: 0.6543 - val_accuracy: 0.6130\n",
      "Epoch 24/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6325 - val_loss: 0.6517 - val_accuracy: 0.6122\n",
      "Epoch 25/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6318 - val_loss: 0.6513 - val_accuracy: 0.6122\n",
      "Epoch 26/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6347 - val_loss: 0.6479 - val_accuracy: 0.6224\n",
      "Epoch 27/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6367 - accuracy: 0.6323 - val_loss: 0.6447 - val_accuracy: 0.6161\n",
      "Epoch 28/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6340 - val_loss: 0.6450 - val_accuracy: 0.6193\n",
      "Epoch 29/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6382 - val_loss: 0.6441 - val_accuracy: 0.6240\n",
      "Epoch 30/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6447 - val_loss: 0.6404 - val_accuracy: 0.6295\n",
      "Epoch 31/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6437 - val_loss: 0.6422 - val_accuracy: 0.6216\n",
      "Epoch 32/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.6455 - val_loss: 0.6382 - val_accuracy: 0.6351\n",
      "Epoch 33/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.6488 - val_loss: 0.6389 - val_accuracy: 0.6351\n",
      "Epoch 34/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6228 - accuracy: 0.6479 - val_loss: 0.6342 - val_accuracy: 0.6303\n",
      "Epoch 35/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.6470 - val_loss: 0.6339 - val_accuracy: 0.6367\n",
      "Epoch 36/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6190 - accuracy: 0.6571 - val_loss: 0.6334 - val_accuracy: 0.6367\n",
      "Epoch 37/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.6522 - val_loss: 0.6321 - val_accuracy: 0.6438\n",
      "Epoch 38/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6600 - val_loss: 0.6282 - val_accuracy: 0.6422\n",
      "Epoch 39/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6612 - val_loss: 0.6278 - val_accuracy: 0.6398\n",
      "Epoch 40/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6636 - val_loss: 0.6253 - val_accuracy: 0.6382\n",
      "Epoch 41/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6085 - accuracy: 0.6713 - val_loss: 0.6295 - val_accuracy: 0.6382\n",
      "Epoch 42/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6085 - accuracy: 0.6668 - val_loss: 0.6247 - val_accuracy: 0.6469\n",
      "Epoch 43/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6120 - accuracy: 0.6560 - val_loss: 0.6222 - val_accuracy: 0.6509\n",
      "Epoch 44/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6033 - accuracy: 0.6675 - val_loss: 0.6220 - val_accuracy: 0.6493\n",
      "Epoch 45/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6057 - accuracy: 0.6643 - val_loss: 0.6164 - val_accuracy: 0.6477\n",
      "Epoch 46/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.6708 - val_loss: 0.6158 - val_accuracy: 0.6524\n",
      "Epoch 47/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.6697 - val_loss: 0.6187 - val_accuracy: 0.6564\n",
      "Epoch 48/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.6721 - val_loss: 0.6137 - val_accuracy: 0.6509\n",
      "Epoch 49/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5953 - accuracy: 0.6768 - val_loss: 0.6148 - val_accuracy: 0.6627\n",
      "Epoch 50/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5952 - accuracy: 0.6771 - val_loss: 0.6125 - val_accuracy: 0.6517\n",
      "Epoch 51/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5902 - accuracy: 0.6809 - val_loss: 0.6134 - val_accuracy: 0.6517\n",
      "Epoch 52/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5915 - accuracy: 0.6794 - val_loss: 0.6086 - val_accuracy: 0.6596\n",
      "Epoch 53/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5871 - accuracy: 0.6849 - val_loss: 0.6085 - val_accuracy: 0.6517\n",
      "Epoch 54/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5859 - accuracy: 0.6876 - val_loss: 0.6095 - val_accuracy: 0.6596\n",
      "Epoch 55/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5860 - accuracy: 0.6877 - val_loss: 0.6074 - val_accuracy: 0.6611\n",
      "Epoch 56/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5827 - accuracy: 0.6872 - val_loss: 0.6047 - val_accuracy: 0.6619\n",
      "Epoch 57/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5811 - accuracy: 0.6923 - val_loss: 0.6067 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5802 - accuracy: 0.6890 - val_loss: 0.6036 - val_accuracy: 0.6603\n",
      "Epoch 59/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5789 - accuracy: 0.6898 - val_loss: 0.6029 - val_accuracy: 0.6698\n",
      "Epoch 60/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5720 - accuracy: 0.6969 - val_loss: 0.6014 - val_accuracy: 0.6682\n",
      "Epoch 61/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.6972 - val_loss: 0.6040 - val_accuracy: 0.6619\n",
      "Epoch 62/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5708 - accuracy: 0.6992 - val_loss: 0.6021 - val_accuracy: 0.6690\n",
      "Epoch 63/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5694 - accuracy: 0.7013 - val_loss: 0.6017 - val_accuracy: 0.6667\n",
      "Epoch 64/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5690 - accuracy: 0.7058 - val_loss: 0.6012 - val_accuracy: 0.6659\n",
      "Epoch 65/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5645 - accuracy: 0.7043 - val_loss: 0.6029 - val_accuracy: 0.6738\n",
      "Epoch 66/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7019 - val_loss: 0.5975 - val_accuracy: 0.6659\n",
      "Epoch 67/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5667 - accuracy: 0.7001 - val_loss: 0.5979 - val_accuracy: 0.6840\n",
      "Epoch 68/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5666 - accuracy: 0.7007 - val_loss: 0.5950 - val_accuracy: 0.6682\n",
      "Epoch 69/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5627 - accuracy: 0.7069 - val_loss: 0.5959 - val_accuracy: 0.6682\n",
      "Epoch 70/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5574 - accuracy: 0.7118 - val_loss: 0.6027 - val_accuracy: 0.6690\n",
      "Epoch 71/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7161 - val_loss: 0.5950 - val_accuracy: 0.6769\n",
      "Epoch 72/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.7132 - val_loss: 0.5976 - val_accuracy: 0.6722\n",
      "Epoch 73/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5559 - accuracy: 0.7121 - val_loss: 0.5930 - val_accuracy: 0.6793\n",
      "Epoch 74/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7145 - val_loss: 0.5941 - val_accuracy: 0.6769\n",
      "Epoch 75/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7111 - val_loss: 0.5969 - val_accuracy: 0.6682\n",
      "Epoch 76/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5463 - accuracy: 0.7190 - val_loss: 0.5884 - val_accuracy: 0.6730\n",
      "Epoch 77/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5473 - accuracy: 0.7170 - val_loss: 0.5906 - val_accuracy: 0.6809\n",
      "Epoch 78/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5467 - accuracy: 0.7174 - val_loss: 0.5892 - val_accuracy: 0.6785\n",
      "Epoch 79/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5400 - accuracy: 0.7220 - val_loss: 0.5898 - val_accuracy: 0.6825\n",
      "Epoch 80/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5450 - accuracy: 0.7201 - val_loss: 0.5866 - val_accuracy: 0.6769\n",
      "Epoch 81/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7214 - val_loss: 0.5848 - val_accuracy: 0.6809\n",
      "Epoch 82/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5412 - accuracy: 0.7250 - val_loss: 0.5886 - val_accuracy: 0.6746\n",
      "Epoch 83/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7305 - val_loss: 0.5896 - val_accuracy: 0.6754\n",
      "Epoch 84/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7302 - val_loss: 0.5915 - val_accuracy: 0.6698\n",
      "Epoch 85/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7289 - val_loss: 0.5850 - val_accuracy: 0.6706\n",
      "Epoch 86/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5305 - accuracy: 0.7312 - val_loss: 0.5870 - val_accuracy: 0.6643\n",
      "Epoch 87/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5293 - accuracy: 0.7365 - val_loss: 0.5852 - val_accuracy: 0.6690\n",
      "Epoch 88/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7373 - val_loss: 0.5788 - val_accuracy: 0.6825\n",
      "Epoch 89/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5280 - accuracy: 0.7358 - val_loss: 0.5880 - val_accuracy: 0.6761\n",
      "Epoch 90/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7347 - val_loss: 0.5835 - val_accuracy: 0.6690\n",
      "Epoch 91/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.7347 - val_loss: 0.5839 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7390 - val_loss: 0.5811 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5209 - accuracy: 0.7385 - val_loss: 0.5878 - val_accuracy: 0.6833\n",
      "Epoch 94/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7414 - val_loss: 0.5790 - val_accuracy: 0.6809\n",
      "Epoch 95/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7366 - val_loss: 0.5812 - val_accuracy: 0.6880\n",
      "Epoch 96/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5173 - accuracy: 0.7397 - val_loss: 0.5780 - val_accuracy: 0.6848\n",
      "Epoch 97/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7433 - val_loss: 0.5768 - val_accuracy: 0.6912\n",
      "Epoch 98/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5062 - accuracy: 0.7505 - val_loss: 0.5852 - val_accuracy: 0.6785\n",
      "Epoch 99/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5075 - accuracy: 0.7482 - val_loss: 0.5809 - val_accuracy: 0.6848\n",
      "Epoch 100/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5117 - accuracy: 0.7464 - val_loss: 0.5865 - val_accuracy: 0.6825\n",
      "Epoch 101/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7476 - val_loss: 0.5794 - val_accuracy: 0.6856\n",
      "Epoch 102/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7460 - val_loss: 0.5842 - val_accuracy: 0.6848\n",
      "Epoch 103/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7526 - val_loss: 0.5796 - val_accuracy: 0.6904\n",
      "Epoch 104/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5068 - accuracy: 0.7471 - val_loss: 0.5809 - val_accuracy: 0.6840\n",
      "Epoch 105/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7557 - val_loss: 0.5778 - val_accuracy: 0.6896\n",
      "Epoch 106/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7462 - val_loss: 0.5772 - val_accuracy: 0.6856\n",
      "Epoch 107/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5082 - accuracy: 0.7484 - val_loss: 0.5801 - val_accuracy: 0.6888\n",
      "Epoch 108/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4964 - accuracy: 0.7560 - val_loss: 0.5749 - val_accuracy: 0.6935\n",
      "Epoch 109/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4992 - accuracy: 0.7568 - val_loss: 0.5772 - val_accuracy: 0.6943\n",
      "Epoch 110/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4950 - accuracy: 0.7556 - val_loss: 0.5778 - val_accuracy: 0.6927\n",
      "Epoch 111/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4944 - accuracy: 0.7600 - val_loss: 0.5771 - val_accuracy: 0.6983\n",
      "Epoch 112/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7617 - val_loss: 0.5775 - val_accuracy: 0.6919\n",
      "Epoch 113/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7595 - val_loss: 0.5758 - val_accuracy: 0.6943\n",
      "Epoch 114/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.7573 - val_loss: 0.5772 - val_accuracy: 0.6888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7590 - val_loss: 0.5731 - val_accuracy: 0.6998\n",
      "Epoch 116/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.7662 - val_loss: 0.5795 - val_accuracy: 0.6912\n",
      "Epoch 117/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.7665 - val_loss: 0.5761 - val_accuracy: 0.6927\n",
      "Epoch 118/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4823 - accuracy: 0.7659 - val_loss: 0.5761 - val_accuracy: 0.6983\n",
      "Epoch 119/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7630 - val_loss: 0.5792 - val_accuracy: 0.6927\n",
      "Epoch 120/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4786 - accuracy: 0.7643 - val_loss: 0.5768 - val_accuracy: 0.6991\n",
      "Epoch 121/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.7705 - val_loss: 0.5759 - val_accuracy: 0.7014\n",
      "Epoch 122/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7718 - val_loss: 0.5730 - val_accuracy: 0.6959\n",
      "Epoch 123/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7705 - val_loss: 0.5765 - val_accuracy: 0.7006\n",
      "Epoch 124/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4750 - accuracy: 0.7711 - val_loss: 0.5718 - val_accuracy: 0.6872\n",
      "Epoch 125/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7749 - val_loss: 0.5723 - val_accuracy: 0.6919\n",
      "Epoch 126/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.7752 - val_loss: 0.5785 - val_accuracy: 0.6935\n",
      "Epoch 127/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7745 - val_loss: 0.5770 - val_accuracy: 0.6959\n",
      "Epoch 128/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4736 - accuracy: 0.7700 - val_loss: 0.5717 - val_accuracy: 0.6904\n",
      "Epoch 129/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4673 - accuracy: 0.7737 - val_loss: 0.5796 - val_accuracy: 0.6896\n",
      "Epoch 130/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4628 - accuracy: 0.7783 - val_loss: 0.5761 - val_accuracy: 0.6983\n",
      "Epoch 131/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7755 - val_loss: 0.5771 - val_accuracy: 0.6943\n",
      "Epoch 132/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7745 - val_loss: 0.5762 - val_accuracy: 0.6904\n",
      "Epoch 133/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4601 - accuracy: 0.7780 - val_loss: 0.5770 - val_accuracy: 0.6975\n",
      "Epoch 134/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7768 - val_loss: 0.5709 - val_accuracy: 0.6912\n",
      "Epoch 135/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.7807 - val_loss: 0.5742 - val_accuracy: 0.7062\n",
      "Epoch 136/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.7852 - val_loss: 0.5720 - val_accuracy: 0.7022\n",
      "Epoch 137/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4570 - accuracy: 0.7797 - val_loss: 0.5727 - val_accuracy: 0.7046\n",
      "Epoch 138/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7820 - val_loss: 0.5730 - val_accuracy: 0.6951\n",
      "Epoch 139/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4589 - accuracy: 0.7814 - val_loss: 0.5746 - val_accuracy: 0.6991\n",
      "Epoch 140/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7849 - val_loss: 0.5725 - val_accuracy: 0.7030\n",
      "Epoch 141/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4529 - accuracy: 0.7824 - val_loss: 0.5729 - val_accuracy: 0.6967\n",
      "Epoch 142/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.7864 - val_loss: 0.5821 - val_accuracy: 0.6959\n",
      "Epoch 143/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.7888 - val_loss: 0.5748 - val_accuracy: 0.7038\n",
      "Epoch 144/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7881 - val_loss: 0.5794 - val_accuracy: 0.7014\n",
      "Epoch 145/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7810 - val_loss: 0.5698 - val_accuracy: 0.7077\n",
      "Epoch 146/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.7865 - val_loss: 0.5738 - val_accuracy: 0.7070\n",
      "Epoch 147/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.7881 - val_loss: 0.5768 - val_accuracy: 0.7093\n",
      "Epoch 148/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.7915 - val_loss: 0.5754 - val_accuracy: 0.7014\n",
      "Epoch 149/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.7942 - val_loss: 0.5789 - val_accuracy: 0.6951\n",
      "Epoch 150/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4378 - accuracy: 0.7917 - val_loss: 0.5770 - val_accuracy: 0.6998\n",
      "Epoch 151/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.7895 - val_loss: 0.5716 - val_accuracy: 0.7054\n",
      "Epoch 152/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.7895 - val_loss: 0.5691 - val_accuracy: 0.7046\n",
      "Epoch 153/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4362 - accuracy: 0.7935 - val_loss: 0.5726 - val_accuracy: 0.7101\n",
      "Epoch 154/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.7926 - val_loss: 0.5663 - val_accuracy: 0.7125\n",
      "Epoch 155/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.7964 - val_loss: 0.5711 - val_accuracy: 0.7093\n",
      "Epoch 156/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4281 - accuracy: 0.7989 - val_loss: 0.5690 - val_accuracy: 0.7062\n",
      "Epoch 157/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.7969 - val_loss: 0.5693 - val_accuracy: 0.7030\n",
      "Epoch 158/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4306 - accuracy: 0.7953 - val_loss: 0.5699 - val_accuracy: 0.7022\n",
      "Epoch 159/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4302 - accuracy: 0.7939 - val_loss: 0.5711 - val_accuracy: 0.7070\n",
      "Epoch 160/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4245 - accuracy: 0.8023 - val_loss: 0.5738 - val_accuracy: 0.7109\n",
      "Epoch 161/200\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4233 - accuracy: 0.7985 - val_loss: 0.5721 - val_accuracy: 0.7109\n",
      "Epoch 162/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4282 - accuracy: 0.7972 - val_loss: 0.5688 - val_accuracy: 0.7125\n",
      "Epoch 163/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4261 - accuracy: 0.8031 - val_loss: 0.5756 - val_accuracy: 0.7022\n",
      "Epoch 164/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8042 - val_loss: 0.5673 - val_accuracy: 0.7085\n",
      "Epoch 165/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4171 - accuracy: 0.8027 - val_loss: 0.5684 - val_accuracy: 0.7070\n",
      "Epoch 166/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.7965 - val_loss: 0.5592 - val_accuracy: 0.7188\n",
      "Epoch 167/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4138 - accuracy: 0.8052 - val_loss: 0.5701 - val_accuracy: 0.7093\n",
      "Epoch 168/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4146 - accuracy: 0.8096 - val_loss: 0.5689 - val_accuracy: 0.7117\n",
      "Epoch 169/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4152 - accuracy: 0.8084 - val_loss: 0.5704 - val_accuracy: 0.7109\n",
      "Epoch 170/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4202 - accuracy: 0.8037 - val_loss: 0.5684 - val_accuracy: 0.7141\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4170 - accuracy: 0.8079 - val_loss: 0.5679 - val_accuracy: 0.7077\n",
      "Epoch 172/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.8110 - val_loss: 0.5702 - val_accuracy: 0.7164\n",
      "Epoch 173/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4109 - accuracy: 0.8106 - val_loss: 0.5798 - val_accuracy: 0.7133\n",
      "Epoch 174/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8095 - val_loss: 0.5838 - val_accuracy: 0.7077\n",
      "Epoch 175/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8062 - val_loss: 0.5699 - val_accuracy: 0.7141\n",
      "Epoch 176/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8152 - val_loss: 0.5731 - val_accuracy: 0.7148\n",
      "Epoch 177/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4066 - accuracy: 0.8133 - val_loss: 0.5734 - val_accuracy: 0.7156\n",
      "Epoch 178/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4025 - accuracy: 0.8147 - val_loss: 0.5890 - val_accuracy: 0.7117\n",
      "Epoch 179/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4087 - accuracy: 0.8074 - val_loss: 0.5742 - val_accuracy: 0.7054\n",
      "Epoch 180/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8175 - val_loss: 0.5823 - val_accuracy: 0.7125\n",
      "Epoch 181/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8166 - val_loss: 0.5748 - val_accuracy: 0.7117\n",
      "Epoch 182/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8178 - val_loss: 0.5769 - val_accuracy: 0.7101\n",
      "Epoch 183/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3985 - accuracy: 0.8182 - val_loss: 0.5754 - val_accuracy: 0.7085\n",
      "Epoch 184/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4004 - accuracy: 0.8116 - val_loss: 0.5711 - val_accuracy: 0.7156\n",
      "Epoch 185/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8116 - val_loss: 0.5748 - val_accuracy: 0.7085\n",
      "Epoch 186/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8140 - val_loss: 0.5747 - val_accuracy: 0.7093\n",
      "Epoch 187/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3972 - accuracy: 0.8158 - val_loss: 0.5668 - val_accuracy: 0.7172\n",
      "Epoch 188/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3954 - accuracy: 0.8135 - val_loss: 0.5719 - val_accuracy: 0.7196\n",
      "Epoch 189/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3876 - accuracy: 0.8249 - val_loss: 0.5741 - val_accuracy: 0.7125\n",
      "Epoch 190/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3883 - accuracy: 0.8214 - val_loss: 0.5707 - val_accuracy: 0.7141\n",
      "Epoch 191/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3932 - accuracy: 0.8199 - val_loss: 0.5655 - val_accuracy: 0.7141\n",
      "Epoch 192/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3918 - accuracy: 0.8218 - val_loss: 0.5728 - val_accuracy: 0.7148\n",
      "Epoch 193/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3876 - accuracy: 0.8234 - val_loss: 0.5739 - val_accuracy: 0.7243\n",
      "Epoch 194/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3874 - accuracy: 0.8212 - val_loss: 0.5734 - val_accuracy: 0.7109\n",
      "Epoch 195/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3861 - accuracy: 0.8209 - val_loss: 0.5691 - val_accuracy: 0.7180\n",
      "Epoch 196/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3830 - accuracy: 0.8239 - val_loss: 0.5759 - val_accuracy: 0.7148\n",
      "Epoch 197/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3844 - accuracy: 0.8266 - val_loss: 0.5822 - val_accuracy: 0.7101\n",
      "Epoch 198/200\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3867 - accuracy: 0.8222 - val_loss: 0.5741 - val_accuracy: 0.7117\n",
      "Epoch 199/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8220 - val_loss: 0.5669 - val_accuracy: 0.7148\n",
      "Epoch 200/200\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.8208 - val_loss: 0.5663 - val_accuracy: 0.7188\n",
      "40/40 [==============================] - 0s 873us/step - loss: 0.5663 - accuracy: 0.7188\n",
      "共耗时103.04367303848267秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "#model.load_weights('store_data/dnn_weights')\n",
    "model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=200,batch_size=64,shuffle=True)  # train the model\n",
    "model.save_weights('store_data/dnn_weights')\n",
    "end_time=time.time()\n",
    "val_loss, val_acc =model.evaluate(x_test, y_test)\n",
    "print(f\"共耗时{end_time-start_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('store_data/dnn_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_svm=SVC(C=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    #('pca', PCA()),\n",
    "    ('svc', model_svm)])\n",
    "model.fit(winsorize_and_standarlize(train[features]),train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=accuracy_score(train.label,model.predict(winsorize_and_standarlize(train[features])))\n",
    "b=accuracy_score(test.label,model.predict(winsorize_and_standarlize(test[features])))\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df =winsorize_and_standarlize(train[['AD', 'ADOSC', 'ADX', 'APO', 'AROONDOWN', 'AROONOSC', 'AROONUP', 'BOP',\n",
    "       'CCI', 'CDLDOJI', 'CDLENGULFING', 'CDLHAMMER', 'CDLMORNINGSTAR', 'CMO',\n",
    "       'COL3BLACKCROWS', 'HT', 'MACD', 'MA_Cross', 'MFI', 'MOM', 'MOM_10',\n",
    "       'MOM_120', 'MOM_60', 'OBV', 'PPO', 'ROC', 'RSI', 'SAR', 'SLOWD',\n",
    "       'SLOWK', 'TSF', 'ULTOSC', 'VSTD_20', 'WILLR', 'ret_60', 'skew_60',\n",
    "       'std_120', 'std_20', 'std_40']])  # now the main_df is all the data up to the last 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=main_df.join(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df=winsorize_and_standarlize(test[['AD', 'ADOSC', 'ADX', 'APO', 'AROONDOWN', 'AROONOSC', 'AROONUP', 'BOP',\n",
    "       'CCI', 'CDLDOJI', 'CDLENGULFING', 'CDLHAMMER', 'CDLMORNINGSTAR', 'CMO',\n",
    "       'COL3BLACKCROWS', 'HT', 'MACD', 'MA_Cross', 'MFI', 'MOM', 'MOM_10',\n",
    "       'MOM_120', 'MOM_60', 'OBV', 'PPO', 'ROC', 'RSI', 'SAR', 'SLOWD',\n",
    "       'SLOWK', 'TSF', 'ULTOSC', 'VSTD_20', 'WILLR', 'ret_60', 'skew_60',\n",
    "       'std_120', 'std_20', 'std_40']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df=validation_main_df.join(test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    \n",
    "    buys=[]\n",
    "    sells=[]\n",
    "    \n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "    \n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "#     lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "#     buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "#     sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "SEQ_LEN = 64\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization,RNN\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.1)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "236/236 [==============================] - 17s 72ms/step - loss: 0.7066 - accuracy: 0.5052 - val_loss: 0.6971 - val_accuracy: 0.4728\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 16s 70ms/step - loss: 0.6961 - accuracy: 0.5169 - val_loss: 0.6917 - val_accuracy: 0.5272\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 17s 71ms/step - loss: 0.6938 - accuracy: 0.5180 - val_loss: 0.6919 - val_accuracy: 0.5272\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 17s 70ms/step - loss: 0.6941 - accuracy: 0.5246 - val_loss: 0.7002 - val_accuracy: 0.5272\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 17s 71ms/step - loss: 0.6977 - accuracy: 0.5196 - val_loss: 0.6969 - val_accuracy: 0.5272\n",
      "Epoch 6/200\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.6944 - accuracy: 0.5078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-57a0731e066d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1123\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_x,train_y,validation_data=(validation_x, validation_y),epochs=200)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_x, validation_y)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy\n",
    "#model.save('LSTM.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(winsorize_and_standarlize(train[features]))\n",
    "y_train=np.array(train.label)\n",
    "x_test=np.array(winsorize_and_standarlize(test[features]))\n",
    "y_test=np.array(test.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8855, 39) (8855,) (3796, 39) (3796,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.expand_dims(x_train,axis=2)\n",
    "X_test=np.expand_dims(x_test,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnnmodel():\n",
    "    model=Sequential()\n",
    "    #建立一维卷积层\n",
    "    model.add(Conv1D(filters=32,#建立16个滤镜\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                    ))\n",
    "    #建立一维池化层1\n",
    "    model.add(MaxPooling1D(pool_size=3)) \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(filters=32,#建立16个滤镜\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                    ))\n",
    "    model.add(MaxPooling1D(pool_size=3)) \n",
    "    model.add(Conv1D(filters=32,#建立16个滤镜\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                    ))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.3))\n",
    "    #建立展平层\n",
    "    model.add(Flatten())\n",
    "    #建立隐藏层\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    #随机放弃25%的神经元\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #输出层\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.001,decay=1e-6),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5884 - accuracy: 0.6827 - val_loss: 0.6470 - val_accuracy: 0.6258\n",
      "Epoch 2/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5912 - accuracy: 0.6745 - val_loss: 0.6439 - val_accuracy: 0.6236\n",
      "Epoch 3/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5881 - accuracy: 0.6885 - val_loss: 0.6397 - val_accuracy: 0.6345\n",
      "Epoch 4/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5897 - accuracy: 0.6831 - val_loss: 0.6393 - val_accuracy: 0.6325\n",
      "Epoch 5/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5870 - accuracy: 0.6804 - val_loss: 0.6398 - val_accuracy: 0.6291\n",
      "Epoch 6/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5892 - accuracy: 0.6779 - val_loss: 0.6387 - val_accuracy: 0.6287\n",
      "Epoch 7/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5938 - accuracy: 0.6759 - val_loss: 0.6394 - val_accuracy: 0.6297\n",
      "Epoch 8/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5919 - accuracy: 0.6784 - val_loss: 0.6354 - val_accuracy: 0.6341\n",
      "Epoch 9/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5897 - accuracy: 0.6826 - val_loss: 0.6376 - val_accuracy: 0.6289\n",
      "Epoch 10/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5893 - accuracy: 0.6833 - val_loss: 0.6378 - val_accuracy: 0.6368\n",
      "Epoch 11/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5904 - accuracy: 0.6778 - val_loss: 0.6370 - val_accuracy: 0.6341\n",
      "Epoch 12/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5885 - accuracy: 0.6845 - val_loss: 0.6366 - val_accuracy: 0.6347\n",
      "Epoch 13/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5849 - accuracy: 0.6863 - val_loss: 0.6399 - val_accuracy: 0.6285\n",
      "Epoch 14/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5880 - accuracy: 0.6809 - val_loss: 0.6406 - val_accuracy: 0.6268\n",
      "Epoch 15/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5861 - accuracy: 0.6826 - val_loss: 0.6380 - val_accuracy: 0.6333\n",
      "Epoch 16/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5937 - accuracy: 0.6797 - val_loss: 0.6350 - val_accuracy: 0.6380\n",
      "Epoch 17/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5877 - accuracy: 0.6772 - val_loss: 0.6349 - val_accuracy: 0.6309\n",
      "Epoch 18/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5913 - accuracy: 0.6798 - val_loss: 0.6397 - val_accuracy: 0.6293\n",
      "Epoch 19/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5921 - accuracy: 0.6765 - val_loss: 0.6338 - val_accuracy: 0.6347\n",
      "Epoch 20/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5924 - accuracy: 0.6704 - val_loss: 0.6355 - val_accuracy: 0.6394\n",
      "Epoch 21/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5913 - accuracy: 0.6800 - val_loss: 0.6381 - val_accuracy: 0.6376\n",
      "Epoch 22/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5959 - accuracy: 0.6700 - val_loss: 0.6350 - val_accuracy: 0.6343\n",
      "Epoch 23/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5902 - accuracy: 0.6753 - val_loss: 0.6307 - val_accuracy: 0.6388\n",
      "Epoch 24/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5940 - accuracy: 0.6740 - val_loss: 0.6320 - val_accuracy: 0.6352\n",
      "Epoch 25/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5882 - accuracy: 0.6837 - val_loss: 0.6316 - val_accuracy: 0.6443\n",
      "Epoch 26/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5850 - accuracy: 0.6836 - val_loss: 0.6317 - val_accuracy: 0.6441\n",
      "Epoch 27/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5876 - accuracy: 0.6751 - val_loss: 0.6326 - val_accuracy: 0.6402\n",
      "Epoch 28/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5851 - accuracy: 0.6800 - val_loss: 0.6301 - val_accuracy: 0.6434\n",
      "Epoch 29/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5913 - accuracy: 0.6761 - val_loss: 0.6305 - val_accuracy: 0.6362\n",
      "Epoch 30/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5834 - accuracy: 0.6855 - val_loss: 0.6312 - val_accuracy: 0.6384\n",
      "Epoch 31/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5922 - accuracy: 0.6826 - val_loss: 0.6291 - val_accuracy: 0.6400\n",
      "Epoch 32/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5905 - accuracy: 0.6813 - val_loss: 0.6293 - val_accuracy: 0.6358\n",
      "Epoch 33/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5891 - accuracy: 0.6827 - val_loss: 0.6272 - val_accuracy: 0.6406\n",
      "Epoch 34/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5905 - accuracy: 0.6766 - val_loss: 0.6322 - val_accuracy: 0.6339\n",
      "Epoch 35/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5842 - accuracy: 0.6880 - val_loss: 0.6301 - val_accuracy: 0.6388\n",
      "Epoch 36/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5921 - accuracy: 0.6720 - val_loss: 0.6305 - val_accuracy: 0.6414\n",
      "Epoch 37/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.6883 - val_loss: 0.6285 - val_accuracy: 0.6406\n",
      "Epoch 38/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5810 - accuracy: 0.6885 - val_loss: 0.6275 - val_accuracy: 0.6392\n",
      "Epoch 39/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5931 - accuracy: 0.6779 - val_loss: 0.6278 - val_accuracy: 0.6396\n",
      "Epoch 40/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5874 - accuracy: 0.6768 - val_loss: 0.6275 - val_accuracy: 0.6392\n",
      "Epoch 41/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5901 - accuracy: 0.6820 - val_loss: 0.6306 - val_accuracy: 0.6349\n",
      "Epoch 42/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.6891 - val_loss: 0.6276 - val_accuracy: 0.6435\n",
      "Epoch 43/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5900 - accuracy: 0.6762 - val_loss: 0.6281 - val_accuracy: 0.6422\n",
      "Epoch 44/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5926 - accuracy: 0.6793 - val_loss: 0.6272 - val_accuracy: 0.6459\n",
      "Epoch 45/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5904 - accuracy: 0.6794 - val_loss: 0.6281 - val_accuracy: 0.6432\n",
      "Epoch 46/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5917 - accuracy: 0.6765 - val_loss: 0.6294 - val_accuracy: 0.6384\n",
      "Epoch 47/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5902 - accuracy: 0.6786 - val_loss: 0.6256 - val_accuracy: 0.6420\n",
      "Epoch 48/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5867 - accuracy: 0.6781 - val_loss: 0.6274 - val_accuracy: 0.6354\n",
      "Epoch 49/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5921 - accuracy: 0.6807 - val_loss: 0.6273 - val_accuracy: 0.6384\n",
      "Epoch 50/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5855 - accuracy: 0.6844 - val_loss: 0.6268 - val_accuracy: 0.6463\n",
      "Epoch 51/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5872 - accuracy: 0.6839 - val_loss: 0.6243 - val_accuracy: 0.6408\n",
      "Epoch 52/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5916 - accuracy: 0.6762 - val_loss: 0.6231 - val_accuracy: 0.6435\n",
      "Epoch 53/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5915 - accuracy: 0.6763 - val_loss: 0.6271 - val_accuracy: 0.6443\n",
      "Epoch 54/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5810 - accuracy: 0.6888 - val_loss: 0.6260 - val_accuracy: 0.6396\n",
      "Epoch 55/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5818 - accuracy: 0.6789 - val_loss: 0.6262 - val_accuracy: 0.6356\n",
      "Epoch 56/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5847 - accuracy: 0.6858 - val_loss: 0.6263 - val_accuracy: 0.6463\n",
      "Epoch 57/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6880 - val_loss: 0.6245 - val_accuracy: 0.6465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5883 - accuracy: 0.6835 - val_loss: 0.6270 - val_accuracy: 0.6406\n",
      "Epoch 59/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.6890 - val_loss: 0.6264 - val_accuracy: 0.6467\n",
      "Epoch 60/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5844 - accuracy: 0.6873 - val_loss: 0.6253 - val_accuracy: 0.6485\n",
      "Epoch 61/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5867 - accuracy: 0.6788 - val_loss: 0.6263 - val_accuracy: 0.6428\n",
      "Epoch 62/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5911 - accuracy: 0.6789 - val_loss: 0.6235 - val_accuracy: 0.6451\n",
      "Epoch 63/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5882 - accuracy: 0.6813 - val_loss: 0.6264 - val_accuracy: 0.6396\n",
      "Epoch 64/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5811 - accuracy: 0.6893 - val_loss: 0.6219 - val_accuracy: 0.6463\n",
      "Epoch 65/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5898 - accuracy: 0.6785 - val_loss: 0.6225 - val_accuracy: 0.6499\n",
      "Epoch 66/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5851 - accuracy: 0.6811 - val_loss: 0.6230 - val_accuracy: 0.6426\n",
      "Epoch 67/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5837 - accuracy: 0.6829 - val_loss: 0.6213 - val_accuracy: 0.6497\n",
      "Epoch 68/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5815 - accuracy: 0.6832 - val_loss: 0.6247 - val_accuracy: 0.6439\n",
      "Epoch 69/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.6870 - val_loss: 0.6236 - val_accuracy: 0.6439\n",
      "Epoch 70/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5884 - accuracy: 0.6821 - val_loss: 0.6210 - val_accuracy: 0.6501\n",
      "Epoch 71/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5873 - accuracy: 0.6812 - val_loss: 0.6240 - val_accuracy: 0.6451\n",
      "Epoch 72/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5895 - accuracy: 0.6780 - val_loss: 0.6224 - val_accuracy: 0.6435\n",
      "Epoch 73/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5845 - accuracy: 0.6861 - val_loss: 0.6219 - val_accuracy: 0.6455\n",
      "Epoch 74/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.6871 - val_loss: 0.6252 - val_accuracy: 0.6404\n",
      "Epoch 75/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5847 - accuracy: 0.6804 - val_loss: 0.6260 - val_accuracy: 0.6410\n",
      "Epoch 76/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5882 - accuracy: 0.6796 - val_loss: 0.6224 - val_accuracy: 0.6495\n",
      "Epoch 77/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5827 - accuracy: 0.6779 - val_loss: 0.6231 - val_accuracy: 0.6469\n",
      "Epoch 78/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5858 - accuracy: 0.6823 - val_loss: 0.6217 - val_accuracy: 0.6485\n",
      "Epoch 79/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5830 - accuracy: 0.6792 - val_loss: 0.6213 - val_accuracy: 0.6473\n",
      "Epoch 80/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5867 - accuracy: 0.6788 - val_loss: 0.6211 - val_accuracy: 0.6473\n",
      "Epoch 81/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5867 - accuracy: 0.6813 - val_loss: 0.6238 - val_accuracy: 0.6424\n",
      "Epoch 82/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5840 - accuracy: 0.6796 - val_loss: 0.6210 - val_accuracy: 0.6475\n",
      "Epoch 83/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.6871 - val_loss: 0.6213 - val_accuracy: 0.6459\n",
      "Epoch 84/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5896 - accuracy: 0.6820 - val_loss: 0.6231 - val_accuracy: 0.6447\n",
      "Epoch 85/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5870 - accuracy: 0.6811 - val_loss: 0.6222 - val_accuracy: 0.6465\n",
      "Epoch 86/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5886 - accuracy: 0.6739 - val_loss: 0.6254 - val_accuracy: 0.6394\n",
      "Epoch 87/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5840 - accuracy: 0.6815 - val_loss: 0.6235 - val_accuracy: 0.6483\n",
      "Epoch 88/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5825 - accuracy: 0.6840 - val_loss: 0.6262 - val_accuracy: 0.6434\n",
      "Epoch 89/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5846 - accuracy: 0.6859 - val_loss: 0.6235 - val_accuracy: 0.6439\n",
      "Epoch 90/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5863 - accuracy: 0.6803 - val_loss: 0.6213 - val_accuracy: 0.6495\n",
      "Epoch 91/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5783 - accuracy: 0.6858 - val_loss: 0.6205 - val_accuracy: 0.6507\n",
      "Epoch 92/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5857 - accuracy: 0.6787 - val_loss: 0.6227 - val_accuracy: 0.6382\n",
      "Epoch 93/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5825 - accuracy: 0.6850 - val_loss: 0.6227 - val_accuracy: 0.6491\n",
      "Epoch 94/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5859 - accuracy: 0.6826 - val_loss: 0.6246 - val_accuracy: 0.6477\n",
      "Epoch 95/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5846 - accuracy: 0.6804 - val_loss: 0.6219 - val_accuracy: 0.6491\n",
      "Epoch 96/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5838 - accuracy: 0.6840 - val_loss: 0.6243 - val_accuracy: 0.6453\n",
      "Epoch 97/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5839 - accuracy: 0.6838 - val_loss: 0.6240 - val_accuracy: 0.6469\n",
      "Epoch 98/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5891 - accuracy: 0.6810 - val_loss: 0.6225 - val_accuracy: 0.6495\n",
      "Epoch 99/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5873 - accuracy: 0.6828 - val_loss: 0.6193 - val_accuracy: 0.6477\n",
      "Epoch 100/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5881 - accuracy: 0.6832 - val_loss: 0.6233 - val_accuracy: 0.6477\n",
      "Epoch 101/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5899 - accuracy: 0.6798 - val_loss: 0.6228 - val_accuracy: 0.6475\n",
      "Epoch 102/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5880 - accuracy: 0.6805 - val_loss: 0.6213 - val_accuracy: 0.6501\n",
      "Epoch 103/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5853 - accuracy: 0.6872 - val_loss: 0.6217 - val_accuracy: 0.6469\n",
      "Epoch 104/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5865 - accuracy: 0.6818 - val_loss: 0.6219 - val_accuracy: 0.6499\n",
      "Epoch 105/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5887 - accuracy: 0.6839 - val_loss: 0.6201 - val_accuracy: 0.6471\n",
      "Epoch 106/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5810 - accuracy: 0.6879 - val_loss: 0.6198 - val_accuracy: 0.6513\n",
      "Epoch 107/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6917 - val_loss: 0.6211 - val_accuracy: 0.6503\n",
      "Epoch 108/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5844 - accuracy: 0.6780 - val_loss: 0.6199 - val_accuracy: 0.6439\n",
      "Epoch 109/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5780 - accuracy: 0.6887 - val_loss: 0.6184 - val_accuracy: 0.6471\n",
      "Epoch 110/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5838 - accuracy: 0.6806 - val_loss: 0.6186 - val_accuracy: 0.6499\n",
      "Epoch 111/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5854 - accuracy: 0.6809 - val_loss: 0.6203 - val_accuracy: 0.6497\n",
      "Epoch 112/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5754 - accuracy: 0.6950 - val_loss: 0.6173 - val_accuracy: 0.6526\n",
      "Epoch 113/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5796 - accuracy: 0.6881 - val_loss: 0.6192 - val_accuracy: 0.6532\n",
      "Epoch 114/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5854 - accuracy: 0.6772 - val_loss: 0.6207 - val_accuracy: 0.6495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5820 - accuracy: 0.6814 - val_loss: 0.6203 - val_accuracy: 0.6435\n",
      "Epoch 116/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5796 - accuracy: 0.6894 - val_loss: 0.6186 - val_accuracy: 0.6522\n",
      "Epoch 117/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5836 - accuracy: 0.6896 - val_loss: 0.6190 - val_accuracy: 0.6483\n",
      "Epoch 118/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5823 - accuracy: 0.6882 - val_loss: 0.6184 - val_accuracy: 0.6530\n",
      "Epoch 119/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5852 - accuracy: 0.6827 - val_loss: 0.6193 - val_accuracy: 0.6572\n",
      "Epoch 120/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5756 - accuracy: 0.6861 - val_loss: 0.6187 - val_accuracy: 0.6501\n",
      "Epoch 121/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5831 - accuracy: 0.6830 - val_loss: 0.6188 - val_accuracy: 0.6481\n",
      "Epoch 122/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6831 - val_loss: 0.6206 - val_accuracy: 0.6471\n",
      "Epoch 123/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5842 - accuracy: 0.6830 - val_loss: 0.6186 - val_accuracy: 0.6522\n",
      "Epoch 124/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5855 - accuracy: 0.6840 - val_loss: 0.6178 - val_accuracy: 0.6455\n",
      "Epoch 125/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5823 - accuracy: 0.6911 - val_loss: 0.6209 - val_accuracy: 0.6501\n",
      "Epoch 126/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5808 - accuracy: 0.6850 - val_loss: 0.6186 - val_accuracy: 0.6522\n",
      "Epoch 127/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5869 - accuracy: 0.6823 - val_loss: 0.6188 - val_accuracy: 0.6473\n",
      "Epoch 128/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5828 - accuracy: 0.6865 - val_loss: 0.6207 - val_accuracy: 0.6465\n",
      "Epoch 129/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5896 - accuracy: 0.6823 - val_loss: 0.6229 - val_accuracy: 0.6434\n",
      "Epoch 130/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5849 - accuracy: 0.6853 - val_loss: 0.6214 - val_accuracy: 0.6465\n",
      "Epoch 131/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5832 - accuracy: 0.6802 - val_loss: 0.6203 - val_accuracy: 0.6487\n",
      "Epoch 132/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5833 - accuracy: 0.6820 - val_loss: 0.6198 - val_accuracy: 0.6513\n",
      "Epoch 133/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5845 - accuracy: 0.6885 - val_loss: 0.6203 - val_accuracy: 0.6432\n",
      "Epoch 134/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5814 - accuracy: 0.6846 - val_loss: 0.6170 - val_accuracy: 0.6483\n",
      "Epoch 135/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5818 - accuracy: 0.6815 - val_loss: 0.6205 - val_accuracy: 0.6410\n",
      "Epoch 136/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.6823 - val_loss: 0.6194 - val_accuracy: 0.6428\n",
      "Epoch 137/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5822 - accuracy: 0.6880 - val_loss: 0.6199 - val_accuracy: 0.6437\n",
      "Epoch 138/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5787 - accuracy: 0.6890 - val_loss: 0.6200 - val_accuracy: 0.6430\n",
      "Epoch 139/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5821 - accuracy: 0.6827 - val_loss: 0.6198 - val_accuracy: 0.6465\n",
      "Epoch 140/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5822 - accuracy: 0.6863 - val_loss: 0.6200 - val_accuracy: 0.6501\n",
      "Epoch 141/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5818 - accuracy: 0.6884 - val_loss: 0.6207 - val_accuracy: 0.6461\n",
      "Epoch 142/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5807 - accuracy: 0.6873 - val_loss: 0.6172 - val_accuracy: 0.6483\n",
      "Epoch 143/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5815 - accuracy: 0.6888 - val_loss: 0.6166 - val_accuracy: 0.6459\n",
      "Epoch 144/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5831 - accuracy: 0.6814 - val_loss: 0.6153 - val_accuracy: 0.6515\n",
      "Epoch 145/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5731 - accuracy: 0.6875 - val_loss: 0.6176 - val_accuracy: 0.6483\n",
      "Epoch 146/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5817 - accuracy: 0.6909 - val_loss: 0.6178 - val_accuracy: 0.6487\n",
      "Epoch 147/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5855 - accuracy: 0.6848 - val_loss: 0.6189 - val_accuracy: 0.6414\n",
      "Epoch 148/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5825 - accuracy: 0.6803 - val_loss: 0.6193 - val_accuracy: 0.6483\n",
      "Epoch 149/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5809 - accuracy: 0.6879 - val_loss: 0.6183 - val_accuracy: 0.6475\n",
      "Epoch 150/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.6847 - val_loss: 0.6172 - val_accuracy: 0.6570\n",
      "Epoch 151/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.6866 - val_loss: 0.6167 - val_accuracy: 0.6572\n",
      "Epoch 152/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5795 - accuracy: 0.6901 - val_loss: 0.6180 - val_accuracy: 0.6518\n",
      "Epoch 153/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5800 - accuracy: 0.6888 - val_loss: 0.6200 - val_accuracy: 0.6507\n",
      "Epoch 154/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6842 - val_loss: 0.6177 - val_accuracy: 0.6503\n",
      "Epoch 155/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.6907 - val_loss: 0.6183 - val_accuracy: 0.6548\n",
      "Epoch 156/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5837 - accuracy: 0.6855 - val_loss: 0.6178 - val_accuracy: 0.6507\n",
      "Epoch 157/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6892 - val_loss: 0.6174 - val_accuracy: 0.6461\n",
      "Epoch 158/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5795 - accuracy: 0.6855 - val_loss: 0.6188 - val_accuracy: 0.6499\n",
      "Epoch 159/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5769 - accuracy: 0.6917 - val_loss: 0.6192 - val_accuracy: 0.6485\n",
      "Epoch 160/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5798 - accuracy: 0.6857 - val_loss: 0.6176 - val_accuracy: 0.6570\n",
      "Epoch 161/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.6870 - val_loss: 0.6180 - val_accuracy: 0.6526\n",
      "Epoch 162/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5879 - accuracy: 0.6792 - val_loss: 0.6177 - val_accuracy: 0.6542\n",
      "Epoch 163/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5861 - accuracy: 0.6881 - val_loss: 0.6202 - val_accuracy: 0.6451\n",
      "Epoch 164/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5847 - accuracy: 0.6868 - val_loss: 0.6162 - val_accuracy: 0.6522\n",
      "Epoch 165/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5782 - accuracy: 0.6911 - val_loss: 0.6218 - val_accuracy: 0.6477\n",
      "Epoch 166/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5802 - accuracy: 0.6937 - val_loss: 0.6163 - val_accuracy: 0.6497\n",
      "Epoch 167/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5827 - accuracy: 0.6864 - val_loss: 0.6188 - val_accuracy: 0.6546\n",
      "Epoch 168/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5782 - accuracy: 0.6902 - val_loss: 0.6158 - val_accuracy: 0.6532\n",
      "Epoch 169/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5849 - accuracy: 0.6800 - val_loss: 0.6175 - val_accuracy: 0.6594\n",
      "Epoch 170/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5846 - accuracy: 0.6821 - val_loss: 0.6179 - val_accuracy: 0.6556\n",
      "Epoch 171/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5774 - accuracy: 0.6925 - val_loss: 0.6177 - val_accuracy: 0.6532\n",
      "Epoch 172/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5769 - accuracy: 0.6908 - val_loss: 0.6165 - val_accuracy: 0.6550\n",
      "Epoch 173/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5804 - accuracy: 0.6865 - val_loss: 0.6182 - val_accuracy: 0.6550\n",
      "Epoch 174/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5802 - accuracy: 0.6884 - val_loss: 0.6179 - val_accuracy: 0.6568\n",
      "Epoch 175/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5815 - accuracy: 0.6831 - val_loss: 0.6194 - val_accuracy: 0.6505\n",
      "Epoch 176/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5824 - accuracy: 0.6923 - val_loss: 0.6159 - val_accuracy: 0.6536\n",
      "Epoch 177/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5886 - accuracy: 0.6853 - val_loss: 0.6178 - val_accuracy: 0.6540\n",
      "Epoch 178/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5842 - accuracy: 0.6877 - val_loss: 0.6168 - val_accuracy: 0.6548\n",
      "Epoch 179/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5836 - accuracy: 0.6804 - val_loss: 0.6145 - val_accuracy: 0.6554\n",
      "Epoch 180/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5798 - accuracy: 0.6850 - val_loss: 0.6166 - val_accuracy: 0.6528\n",
      "Epoch 181/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.6850 - val_loss: 0.6178 - val_accuracy: 0.6560\n",
      "Epoch 182/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5886 - accuracy: 0.6823 - val_loss: 0.6184 - val_accuracy: 0.6534\n",
      "Epoch 183/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5824 - accuracy: 0.6861 - val_loss: 0.6151 - val_accuracy: 0.6601\n",
      "Epoch 184/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5821 - accuracy: 0.6811 - val_loss: 0.6172 - val_accuracy: 0.6564\n",
      "Epoch 185/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5773 - accuracy: 0.6862 - val_loss: 0.6174 - val_accuracy: 0.6511\n",
      "Epoch 186/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5861 - accuracy: 0.6806 - val_loss: 0.6138 - val_accuracy: 0.6556\n",
      "Epoch 187/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5787 - accuracy: 0.6944 - val_loss: 0.6150 - val_accuracy: 0.6550\n",
      "Epoch 188/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5823 - accuracy: 0.6861 - val_loss: 0.6148 - val_accuracy: 0.6550\n",
      "Epoch 189/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5822 - accuracy: 0.6894 - val_loss: 0.6137 - val_accuracy: 0.6556\n",
      "Epoch 190/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5776 - accuracy: 0.6949 - val_loss: 0.6152 - val_accuracy: 0.6499\n",
      "Epoch 191/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5762 - accuracy: 0.6905 - val_loss: 0.6152 - val_accuracy: 0.6495\n",
      "Epoch 192/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5794 - accuracy: 0.6889 - val_loss: 0.6188 - val_accuracy: 0.6532\n",
      "Epoch 193/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5782 - accuracy: 0.6891 - val_loss: 0.6175 - val_accuracy: 0.6483\n",
      "Epoch 194/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.6959 - val_loss: 0.6144 - val_accuracy: 0.6558\n",
      "Epoch 195/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5830 - accuracy: 0.6841 - val_loss: 0.6129 - val_accuracy: 0.6584\n",
      "Epoch 196/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.6957 - val_loss: 0.6136 - val_accuracy: 0.6540\n",
      "Epoch 197/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5785 - accuracy: 0.6925 - val_loss: 0.6181 - val_accuracy: 0.6554\n",
      "Epoch 198/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5830 - accuracy: 0.6845 - val_loss: 0.6164 - val_accuracy: 0.6522\n",
      "Epoch 199/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6918 - val_loss: 0.6152 - val_accuracy: 0.6501\n",
      "Epoch 200/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6856 - val_loss: 0.6145 - val_accuracy: 0.6564\n",
      "Epoch 201/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5834 - accuracy: 0.6857 - val_loss: 0.6153 - val_accuracy: 0.6633\n",
      "Epoch 202/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5790 - accuracy: 0.6890 - val_loss: 0.6212 - val_accuracy: 0.6497\n",
      "Epoch 203/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5797 - accuracy: 0.6923 - val_loss: 0.6145 - val_accuracy: 0.6613\n",
      "Epoch 204/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5766 - accuracy: 0.6883 - val_loss: 0.6140 - val_accuracy: 0.6596\n",
      "Epoch 205/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5743 - accuracy: 0.6947 - val_loss: 0.6167 - val_accuracy: 0.6562\n",
      "Epoch 206/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5790 - accuracy: 0.6891 - val_loss: 0.6175 - val_accuracy: 0.6475\n",
      "Epoch 207/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5770 - accuracy: 0.6888 - val_loss: 0.6122 - val_accuracy: 0.6540\n",
      "Epoch 208/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5813 - accuracy: 0.6882 - val_loss: 0.6140 - val_accuracy: 0.6586\n",
      "Epoch 209/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5851 - accuracy: 0.6855 - val_loss: 0.6164 - val_accuracy: 0.6548\n",
      "Epoch 210/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5716 - accuracy: 0.6905 - val_loss: 0.6160 - val_accuracy: 0.6582\n",
      "Epoch 211/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.6917 - val_loss: 0.6153 - val_accuracy: 0.6550\n",
      "Epoch 212/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5810 - accuracy: 0.6873 - val_loss: 0.6136 - val_accuracy: 0.6552\n",
      "Epoch 213/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5803 - accuracy: 0.6880 - val_loss: 0.6143 - val_accuracy: 0.6572\n",
      "Epoch 214/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5801 - accuracy: 0.6875 - val_loss: 0.6140 - val_accuracy: 0.6584\n",
      "Epoch 215/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5800 - accuracy: 0.6833 - val_loss: 0.6126 - val_accuracy: 0.6637\n",
      "Epoch 216/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6881 - val_loss: 0.6128 - val_accuracy: 0.6609\n",
      "Epoch 217/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.6839 - val_loss: 0.6132 - val_accuracy: 0.6601\n",
      "Epoch 218/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.6861 - val_loss: 0.6142 - val_accuracy: 0.6611\n",
      "Epoch 219/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5803 - accuracy: 0.6855 - val_loss: 0.6149 - val_accuracy: 0.6534\n",
      "Epoch 220/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6846 - val_loss: 0.6163 - val_accuracy: 0.6495\n",
      "Epoch 221/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5751 - accuracy: 0.6899 - val_loss: 0.6119 - val_accuracy: 0.6625\n",
      "Epoch 222/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5736 - accuracy: 0.6940 - val_loss: 0.6091 - val_accuracy: 0.6688\n",
      "Epoch 223/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.6960 - val_loss: 0.6154 - val_accuracy: 0.6556\n",
      "Epoch 224/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5758 - accuracy: 0.6952 - val_loss: 0.6131 - val_accuracy: 0.6576\n",
      "Epoch 225/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5750 - accuracy: 0.6910 - val_loss: 0.6165 - val_accuracy: 0.6578\n",
      "Epoch 226/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.6922 - val_loss: 0.6142 - val_accuracy: 0.6592\n",
      "Epoch 227/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5804 - accuracy: 0.6916 - val_loss: 0.6158 - val_accuracy: 0.6564\n",
      "Epoch 228/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5795 - accuracy: 0.6898 - val_loss: 0.6146 - val_accuracy: 0.6566\n",
      "Epoch 229/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5809 - accuracy: 0.6837 - val_loss: 0.6166 - val_accuracy: 0.6536\n",
      "Epoch 230/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5835 - accuracy: 0.6872 - val_loss: 0.6138 - val_accuracy: 0.6633\n",
      "Epoch 231/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6890 - val_loss: 0.6180 - val_accuracy: 0.6564\n",
      "Epoch 232/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6851 - val_loss: 0.6139 - val_accuracy: 0.6590\n",
      "Epoch 233/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5814 - accuracy: 0.6870 - val_loss: 0.6166 - val_accuracy: 0.6548\n",
      "Epoch 234/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5801 - accuracy: 0.6901 - val_loss: 0.6123 - val_accuracy: 0.6601\n",
      "Epoch 235/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5749 - accuracy: 0.6952 - val_loss: 0.6122 - val_accuracy: 0.6613\n",
      "Epoch 236/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5744 - accuracy: 0.6917 - val_loss: 0.6153 - val_accuracy: 0.6552\n",
      "Epoch 237/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5742 - accuracy: 0.6943 - val_loss: 0.6144 - val_accuracy: 0.6621\n",
      "Epoch 238/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5739 - accuracy: 0.6979 - val_loss: 0.6117 - val_accuracy: 0.6627\n",
      "Epoch 239/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.6962 - val_loss: 0.6133 - val_accuracy: 0.6613\n",
      "Epoch 240/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - accuracy: 0.6914 - val_loss: 0.6143 - val_accuracy: 0.6596\n",
      "Epoch 241/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6882 - val_loss: 0.6155 - val_accuracy: 0.6572\n",
      "Epoch 242/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.6894 - val_loss: 0.6116 - val_accuracy: 0.6566\n",
      "Epoch 243/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.6911 - val_loss: 0.6123 - val_accuracy: 0.6572\n",
      "Epoch 244/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6919 - val_loss: 0.6120 - val_accuracy: 0.6605\n",
      "Epoch 245/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.6911 - val_loss: 0.6148 - val_accuracy: 0.6605\n",
      "Epoch 246/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.6935 - val_loss: 0.6139 - val_accuracy: 0.6596\n",
      "Epoch 247/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.6928 - val_loss: 0.6130 - val_accuracy: 0.6574\n",
      "Epoch 248/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.6955 - val_loss: 0.6126 - val_accuracy: 0.6617\n",
      "Epoch 249/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5830 - accuracy: 0.6872 - val_loss: 0.6139 - val_accuracy: 0.6582\n",
      "Epoch 250/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5800 - accuracy: 0.6854 - val_loss: 0.6155 - val_accuracy: 0.6603\n",
      "Epoch 251/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5848 - accuracy: 0.6839 - val_loss: 0.6128 - val_accuracy: 0.6607\n",
      "Epoch 252/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5773 - accuracy: 0.6926 - val_loss: 0.6147 - val_accuracy: 0.6568\n",
      "Epoch 253/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.6909 - val_loss: 0.6165 - val_accuracy: 0.6603\n",
      "Epoch 254/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5807 - accuracy: 0.6822 - val_loss: 0.6155 - val_accuracy: 0.6578\n",
      "Epoch 255/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5724 - accuracy: 0.6890 - val_loss: 0.6129 - val_accuracy: 0.6621\n",
      "Epoch 256/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5803 - accuracy: 0.6879 - val_loss: 0.6156 - val_accuracy: 0.6599\n",
      "Epoch 257/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5841 - accuracy: 0.6804 - val_loss: 0.6156 - val_accuracy: 0.6560\n",
      "Epoch 258/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.6943 - val_loss: 0.6121 - val_accuracy: 0.6615\n",
      "Epoch 259/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.6868 - val_loss: 0.6154 - val_accuracy: 0.6592\n",
      "Epoch 260/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5735 - accuracy: 0.6924 - val_loss: 0.6116 - val_accuracy: 0.6601\n",
      "Epoch 261/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5752 - accuracy: 0.6899 - val_loss: 0.6122 - val_accuracy: 0.6607\n",
      "Epoch 262/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5717 - accuracy: 0.6955 - val_loss: 0.6157 - val_accuracy: 0.6560\n",
      "Epoch 263/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5731 - accuracy: 0.6925 - val_loss: 0.6173 - val_accuracy: 0.6495\n",
      "Epoch 264/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.6966 - val_loss: 0.6164 - val_accuracy: 0.6564\n",
      "Epoch 265/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5842 - accuracy: 0.6816 - val_loss: 0.6135 - val_accuracy: 0.6590\n",
      "Epoch 266/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5756 - accuracy: 0.6892 - val_loss: 0.6172 - val_accuracy: 0.6570\n",
      "Epoch 267/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5749 - accuracy: 0.6963 - val_loss: 0.6145 - val_accuracy: 0.6550\n",
      "Epoch 268/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5754 - accuracy: 0.6909 - val_loss: 0.6144 - val_accuracy: 0.6548\n",
      "Epoch 269/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5750 - accuracy: 0.6923 - val_loss: 0.6146 - val_accuracy: 0.6556\n",
      "Epoch 270/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5801 - accuracy: 0.6805 - val_loss: 0.6136 - val_accuracy: 0.6560\n",
      "Epoch 271/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - accuracy: 0.6861 - val_loss: 0.6148 - val_accuracy: 0.6491\n",
      "Epoch 272/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6958 - val_loss: 0.6141 - val_accuracy: 0.6538\n",
      "Epoch 273/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.6848 - val_loss: 0.6131 - val_accuracy: 0.6584\n",
      "Epoch 274/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5701 - accuracy: 0.6934 - val_loss: 0.6105 - val_accuracy: 0.6596\n",
      "Epoch 275/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5754 - accuracy: 0.6888 - val_loss: 0.6152 - val_accuracy: 0.6601\n",
      "Epoch 276/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5818 - accuracy: 0.6850 - val_loss: 0.6147 - val_accuracy: 0.6601\n",
      "Epoch 277/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5757 - accuracy: 0.6899 - val_loss: 0.6147 - val_accuracy: 0.6619\n",
      "Epoch 278/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.6853 - val_loss: 0.6131 - val_accuracy: 0.6590\n",
      "Epoch 279/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.6861 - val_loss: 0.6179 - val_accuracy: 0.6511\n",
      "Epoch 280/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.6927 - val_loss: 0.6155 - val_accuracy: 0.6574\n",
      "Epoch 281/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.6993 - val_loss: 0.6126 - val_accuracy: 0.6582\n",
      "Epoch 282/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5757 - accuracy: 0.6867 - val_loss: 0.6138 - val_accuracy: 0.6590\n",
      "Epoch 283/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5761 - accuracy: 0.6920 - val_loss: 0.6126 - val_accuracy: 0.6599\n",
      "Epoch 284/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.6911 - val_loss: 0.6127 - val_accuracy: 0.6550\n",
      "Epoch 285/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.6884 - val_loss: 0.6116 - val_accuracy: 0.6601\n",
      "Epoch 286/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.6929 - val_loss: 0.6141 - val_accuracy: 0.6615\n",
      "Epoch 287/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5707 - accuracy: 0.6942 - val_loss: 0.6153 - val_accuracy: 0.6568\n",
      "Epoch 288/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5807 - accuracy: 0.6857 - val_loss: 0.6134 - val_accuracy: 0.6556\n",
      "Epoch 289/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5800 - accuracy: 0.6874 - val_loss: 0.6152 - val_accuracy: 0.6629\n",
      "Epoch 290/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5698 - accuracy: 0.6961 - val_loss: 0.6099 - val_accuracy: 0.6574\n",
      "Epoch 291/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.6943 - val_loss: 0.6105 - val_accuracy: 0.6603\n",
      "Epoch 292/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.6942 - val_loss: 0.6132 - val_accuracy: 0.6592\n",
      "Epoch 293/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5844 - accuracy: 0.6863 - val_loss: 0.6135 - val_accuracy: 0.6538\n",
      "Epoch 294/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - accuracy: 0.6891 - val_loss: 0.6101 - val_accuracy: 0.6584\n",
      "Epoch 295/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.6912 - val_loss: 0.6102 - val_accuracy: 0.6586\n",
      "Epoch 296/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5720 - accuracy: 0.6923 - val_loss: 0.6096 - val_accuracy: 0.6578\n",
      "Epoch 297/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.6894 - val_loss: 0.6136 - val_accuracy: 0.6556\n",
      "Epoch 298/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.6875 - val_loss: 0.6134 - val_accuracy: 0.6590\n",
      "Epoch 299/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5786 - accuracy: 0.6842 - val_loss: 0.6140 - val_accuracy: 0.6554\n",
      "Epoch 300/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5791 - accuracy: 0.6877 - val_loss: 0.6132 - val_accuracy: 0.6590\n",
      "Epoch 301/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.6934 - val_loss: 0.6148 - val_accuracy: 0.6556\n",
      "Epoch 302/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.6953 - val_loss: 0.6119 - val_accuracy: 0.6611\n",
      "Epoch 303/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5758 - accuracy: 0.6876 - val_loss: 0.6131 - val_accuracy: 0.6613\n",
      "Epoch 304/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6900 - val_loss: 0.6130 - val_accuracy: 0.6615\n",
      "Epoch 305/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5766 - accuracy: 0.6923 - val_loss: 0.6126 - val_accuracy: 0.6576\n",
      "Epoch 306/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - accuracy: 0.6914 - val_loss: 0.6128 - val_accuracy: 0.6596\n",
      "Epoch 307/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - accuracy: 0.6931 - val_loss: 0.6099 - val_accuracy: 0.6568\n",
      "Epoch 308/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5834 - accuracy: 0.6874 - val_loss: 0.6113 - val_accuracy: 0.6601\n",
      "Epoch 309/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5761 - accuracy: 0.6883 - val_loss: 0.6126 - val_accuracy: 0.6607\n",
      "Epoch 310/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.6905 - val_loss: 0.6136 - val_accuracy: 0.6570\n",
      "Epoch 311/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.6914 - val_loss: 0.6101 - val_accuracy: 0.6607\n",
      "Epoch 312/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5767 - accuracy: 0.6916 - val_loss: 0.6147 - val_accuracy: 0.6586\n",
      "Epoch 313/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5720 - accuracy: 0.6926 - val_loss: 0.6109 - val_accuracy: 0.6599\n",
      "Epoch 314/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5773 - accuracy: 0.6856 - val_loss: 0.6132 - val_accuracy: 0.6615\n",
      "Epoch 315/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6940 - val_loss: 0.6131 - val_accuracy: 0.6540\n",
      "Epoch 316/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.6902 - val_loss: 0.6115 - val_accuracy: 0.6544\n",
      "Epoch 317/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.6950 - val_loss: 0.6160 - val_accuracy: 0.6556\n",
      "Epoch 318/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5744 - accuracy: 0.6902 - val_loss: 0.6134 - val_accuracy: 0.6633\n",
      "Epoch 319/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5761 - accuracy: 0.6914 - val_loss: 0.6127 - val_accuracy: 0.6558\n",
      "Epoch 320/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6890 - val_loss: 0.6112 - val_accuracy: 0.6637\n",
      "Epoch 321/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5747 - accuracy: 0.6941 - val_loss: 0.6099 - val_accuracy: 0.6651\n",
      "Epoch 322/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5735 - accuracy: 0.6935 - val_loss: 0.6093 - val_accuracy: 0.6615\n",
      "Epoch 323/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.6981 - val_loss: 0.6122 - val_accuracy: 0.6635\n",
      "Epoch 324/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5715 - accuracy: 0.6969 - val_loss: 0.6083 - val_accuracy: 0.6675\n",
      "Epoch 325/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5796 - accuracy: 0.6807 - val_loss: 0.6108 - val_accuracy: 0.6625\n",
      "Epoch 326/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5669 - accuracy: 0.6940 - val_loss: 0.6141 - val_accuracy: 0.6611\n",
      "Epoch 327/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5727 - accuracy: 0.6963 - val_loss: 0.6117 - val_accuracy: 0.6594\n",
      "Epoch 328/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5726 - accuracy: 0.6973 - val_loss: 0.6111 - val_accuracy: 0.6619\n",
      "Epoch 329/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - accuracy: 0.6894 - val_loss: 0.6128 - val_accuracy: 0.6576\n",
      "Epoch 330/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.6935 - val_loss: 0.6122 - val_accuracy: 0.6623\n",
      "Epoch 331/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.6889 - val_loss: 0.6109 - val_accuracy: 0.6619\n",
      "Epoch 332/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5731 - accuracy: 0.6856 - val_loss: 0.6118 - val_accuracy: 0.6552\n",
      "Epoch 333/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5788 - accuracy: 0.6875 - val_loss: 0.6112 - val_accuracy: 0.6599\n",
      "Epoch 334/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5763 - accuracy: 0.6940 - val_loss: 0.6119 - val_accuracy: 0.6599\n",
      "Epoch 335/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5778 - accuracy: 0.6803 - val_loss: 0.6124 - val_accuracy: 0.6566\n",
      "Epoch 336/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5766 - accuracy: 0.6901 - val_loss: 0.6122 - val_accuracy: 0.6607\n",
      "Epoch 337/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.6912 - val_loss: 0.6116 - val_accuracy: 0.6673\n",
      "Epoch 338/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - accuracy: 0.6864 - val_loss: 0.6124 - val_accuracy: 0.6647\n",
      "Epoch 339/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5670 - accuracy: 0.6957 - val_loss: 0.6108 - val_accuracy: 0.6627\n",
      "Epoch 340/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6901 - val_loss: 0.6138 - val_accuracy: 0.6663\n",
      "Epoch 341/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5745 - accuracy: 0.6907 - val_loss: 0.6129 - val_accuracy: 0.6568\n",
      "Epoch 342/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.6922 - val_loss: 0.6138 - val_accuracy: 0.6635\n",
      "Epoch 343/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6928 - val_loss: 0.6143 - val_accuracy: 0.6615\n",
      "Epoch 344/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6927 - val_loss: 0.6154 - val_accuracy: 0.6607\n",
      "Epoch 345/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5801 - accuracy: 0.6907 - val_loss: 0.6133 - val_accuracy: 0.6641\n",
      "Epoch 346/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5750 - accuracy: 0.6938 - val_loss: 0.6143 - val_accuracy: 0.6548\n",
      "Epoch 347/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.6826 - val_loss: 0.6152 - val_accuracy: 0.6601\n",
      "Epoch 348/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.6982 - val_loss: 0.6121 - val_accuracy: 0.6603\n",
      "Epoch 349/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5742 - accuracy: 0.6861 - val_loss: 0.6145 - val_accuracy: 0.6560\n",
      "Epoch 350/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5712 - accuracy: 0.7027 - val_loss: 0.6151 - val_accuracy: 0.6601\n",
      "Epoch 351/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5690 - accuracy: 0.6928 - val_loss: 0.6148 - val_accuracy: 0.6536\n",
      "Epoch 352/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5730 - accuracy: 0.6938 - val_loss: 0.6123 - val_accuracy: 0.6601\n",
      "Epoch 353/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5783 - accuracy: 0.6909 - val_loss: 0.6158 - val_accuracy: 0.6617\n",
      "Epoch 354/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - accuracy: 0.6924 - val_loss: 0.6147 - val_accuracy: 0.6605\n",
      "Epoch 355/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5696 - accuracy: 0.6971 - val_loss: 0.6136 - val_accuracy: 0.6554\n",
      "Epoch 356/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5779 - accuracy: 0.6863 - val_loss: 0.6138 - val_accuracy: 0.6631\n",
      "Epoch 357/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5746 - accuracy: 0.6926 - val_loss: 0.6133 - val_accuracy: 0.6582\n",
      "Epoch 358/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5693 - accuracy: 0.6927 - val_loss: 0.6132 - val_accuracy: 0.6605\n",
      "Epoch 359/400\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.6917 - val_loss: 0.6102 - val_accuracy: 0.6635\n",
      "Epoch 360/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5695 - accuracy: 0.6945 - val_loss: 0.6112 - val_accuracy: 0.6599\n",
      "Epoch 361/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.6940 - val_loss: 0.6099 - val_accuracy: 0.6641\n",
      "Epoch 362/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5777 - accuracy: 0.6925 - val_loss: 0.6105 - val_accuracy: 0.6639\n",
      "Epoch 363/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5743 - accuracy: 0.6915 - val_loss: 0.6107 - val_accuracy: 0.6609\n",
      "Epoch 364/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.6907 - val_loss: 0.6107 - val_accuracy: 0.6639\n",
      "Epoch 365/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5704 - accuracy: 0.6994 - val_loss: 0.6114 - val_accuracy: 0.6594\n",
      "Epoch 366/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5778 - accuracy: 0.6888 - val_loss: 0.6102 - val_accuracy: 0.6582\n",
      "Epoch 367/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5725 - accuracy: 0.6975 - val_loss: 0.6122 - val_accuracy: 0.6627\n",
      "Epoch 368/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6873 - val_loss: 0.6121 - val_accuracy: 0.6653\n",
      "Epoch 369/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5681 - accuracy: 0.7004 - val_loss: 0.6139 - val_accuracy: 0.6607\n",
      "Epoch 370/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.6962 - val_loss: 0.6129 - val_accuracy: 0.6592\n",
      "Epoch 371/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5736 - accuracy: 0.6960 - val_loss: 0.6152 - val_accuracy: 0.6613\n",
      "Epoch 372/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5769 - accuracy: 0.6892 - val_loss: 0.6130 - val_accuracy: 0.6633\n",
      "Epoch 373/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5675 - accuracy: 0.6926 - val_loss: 0.6103 - val_accuracy: 0.6619\n",
      "Epoch 374/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5692 - accuracy: 0.7028 - val_loss: 0.6098 - val_accuracy: 0.6665\n",
      "Epoch 375/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.6903 - val_loss: 0.6106 - val_accuracy: 0.6669\n",
      "Epoch 376/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5723 - accuracy: 0.6920 - val_loss: 0.6111 - val_accuracy: 0.6651\n",
      "Epoch 377/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5725 - accuracy: 0.6927 - val_loss: 0.6102 - val_accuracy: 0.6659\n",
      "Epoch 378/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.6935 - val_loss: 0.6083 - val_accuracy: 0.6692\n",
      "Epoch 379/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5776 - accuracy: 0.6901 - val_loss: 0.6135 - val_accuracy: 0.6625\n",
      "Epoch 380/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5686 - accuracy: 0.6985 - val_loss: 0.6118 - val_accuracy: 0.6621\n",
      "Epoch 381/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6934 - val_loss: 0.6110 - val_accuracy: 0.6649\n",
      "Epoch 382/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5743 - accuracy: 0.6952 - val_loss: 0.6083 - val_accuracy: 0.6657\n",
      "Epoch 383/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5726 - accuracy: 0.7006 - val_loss: 0.6102 - val_accuracy: 0.6663\n",
      "Epoch 384/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.6882 - val_loss: 0.6103 - val_accuracy: 0.6635\n",
      "Epoch 385/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6864 - val_loss: 0.6110 - val_accuracy: 0.6609\n",
      "Epoch 386/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5765 - accuracy: 0.6935 - val_loss: 0.6092 - val_accuracy: 0.6682\n",
      "Epoch 387/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5699 - accuracy: 0.6959 - val_loss: 0.6087 - val_accuracy: 0.6679\n",
      "Epoch 388/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5707 - accuracy: 0.6967 - val_loss: 0.6113 - val_accuracy: 0.6649\n",
      "Epoch 389/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5746 - accuracy: 0.6977 - val_loss: 0.6112 - val_accuracy: 0.6643\n",
      "Epoch 390/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5691 - accuracy: 0.6899 - val_loss: 0.6091 - val_accuracy: 0.6712\n",
      "Epoch 391/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5714 - accuracy: 0.6909 - val_loss: 0.6110 - val_accuracy: 0.6657\n",
      "Epoch 392/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5715 - accuracy: 0.6987 - val_loss: 0.6083 - val_accuracy: 0.6663\n",
      "Epoch 393/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5796 - accuracy: 0.6885 - val_loss: 0.6073 - val_accuracy: 0.6647\n",
      "Epoch 394/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5665 - accuracy: 0.6995 - val_loss: 0.6127 - val_accuracy: 0.6603\n",
      "Epoch 395/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6910 - val_loss: 0.6090 - val_accuracy: 0.6702\n",
      "Epoch 396/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5703 - accuracy: 0.6898 - val_loss: 0.6087 - val_accuracy: 0.6643\n",
      "Epoch 397/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5697 - accuracy: 0.6978 - val_loss: 0.6084 - val_accuracy: 0.6661\n",
      "Epoch 398/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.6861 - val_loss: 0.6089 - val_accuracy: 0.6649\n",
      "Epoch 399/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5709 - accuracy: 0.6970 - val_loss: 0.6080 - val_accuracy: 0.6663\n",
      "Epoch 400/400\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5814 - accuracy: 0.6821 - val_loss: 0.6082 - val_accuracy: 0.6665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x279257bbc10>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model3=create_cnnmodel()\n",
    "model3.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8792, 64, 39)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS=64\n",
    "INPUT_DIM=39\n",
    "lstm_units = 64\n",
    "batch_size = 256\n",
    "inputs = Input(shape=(TIME_STEPS, INPUT_DIM))\n",
    "x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)\n",
    "x = MaxPooling1D(pool_size = 3)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Conv1D(filters = 32, kernel_size = 1, activation = 'relu')(inputs)\n",
    "# x = MaxPooling1D(pool_size = 3)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "\n",
    "# x = Conv1D(filters = 32, kernel_size = 1, activation = 'relu')(inputs)\n",
    "# x = MaxPooling1D(pool_size = 3)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "# print(x.shape)\n",
    "lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "lstm_out=Flatten()(lstm_out)\n",
    "output = Dense(10, activation='sigmoid')(lstm_out)\n",
    "output = Dense(2, activation='sigmoid')(output)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.1, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "35/35 [==============================] - 3s 97ms/step - loss: 0.7050 - accuracy: 0.5297 - val_loss: 0.6931 - val_accuracy: 0.5216\n",
      "Epoch 2/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6930 - accuracy: 0.5349 - val_loss: 0.6930 - val_accuracy: 0.5216\n",
      "Epoch 3/60\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.6928 - accuracy: 0.5349 - val_loss: 0.6929 - val_accuracy: 0.5216\n",
      "Epoch 4/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6926 - accuracy: 0.5349 - val_loss: 0.6927 - val_accuracy: 0.5216\n",
      "Epoch 5/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6922 - accuracy: 0.5349 - val_loss: 0.6924 - val_accuracy: 0.5216\n",
      "Epoch 6/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6912 - accuracy: 0.5349 - val_loss: 0.6928 - val_accuracy: 0.5216\n",
      "Epoch 7/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6926 - val_accuracy: 0.5216\n",
      "Epoch 8/60\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6923 - val_accuracy: 0.5216\n",
      "Epoch 9/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6927 - val_accuracy: 0.5216\n",
      "Epoch 10/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 11/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6928 - val_accuracy: 0.5216\n",
      "Epoch 12/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6924 - val_accuracy: 0.5216\n",
      "Epoch 13/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 14/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6907 - accuracy: 0.5349 - val_loss: 0.6933 - val_accuracy: 0.5216\n",
      "Epoch 15/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6923 - val_accuracy: 0.5216\n",
      "Epoch 16/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6912 - accuracy: 0.5349 - val_loss: 0.6928 - val_accuracy: 0.5216\n",
      "Epoch 17/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6925 - val_accuracy: 0.5216\n",
      "Epoch 18/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6936 - val_accuracy: 0.5216\n",
      "Epoch 19/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6907 - accuracy: 0.5349 - val_loss: 0.6924 - val_accuracy: 0.5216\n",
      "Epoch 20/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6926 - val_accuracy: 0.5216\n",
      "Epoch 21/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6912 - accuracy: 0.5349 - val_loss: 0.6943 - val_accuracy: 0.5216\n",
      "Epoch 22/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6924 - val_accuracy: 0.5216\n",
      "Epoch 23/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6928 - val_accuracy: 0.5216\n",
      "Epoch 24/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6931 - val_accuracy: 0.5216\n",
      "Epoch 25/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6928 - val_accuracy: 0.5216\n",
      "Epoch 26/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6924 - val_accuracy: 0.5216\n",
      "Epoch 27/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 28/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6926 - val_accuracy: 0.5216\n",
      "Epoch 29/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6929 - val_accuracy: 0.5216\n",
      "Epoch 30/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 31/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6935 - val_accuracy: 0.5216\n",
      "Epoch 32/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6908 - accuracy: 0.5349 - val_loss: 0.6923 - val_accuracy: 0.5216\n",
      "Epoch 33/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6930 - val_accuracy: 0.5216\n",
      "Epoch 34/60\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6933 - val_accuracy: 0.5216\n",
      "Epoch 35/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6933 - val_accuracy: 0.5216\n",
      "Epoch 36/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6923 - val_accuracy: 0.5216\n",
      "Epoch 37/60\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.6909 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 38/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6912 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 39/60\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6912 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 40/60\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6925 - val_accuracy: 0.5216\n",
      "Epoch 41/60\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.6913 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 42/60\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.6910 - accuracy: 0.5349 - val_loss: 0.6922 - val_accuracy: 0.5216\n",
      "Epoch 43/60\n",
      " 3/35 [=>............................] - ETA: 1s - loss: 0.6900 - accuracy: 0.5495"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-ee96ae0e9b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_x,train_y, validation_data=(validation_x, validation_y),epochs=60, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM+ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 32)\n",
      "(None, 16)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, merge\n",
    "from keras import layers\n",
    "\n",
    "inputs = Input(shape=(TIME_STEPS, INPUT_DIM))\n",
    "x = Conv1D(filters = 32, kernel_size = 3, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "x = MaxPooling1D(pool_size = 3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "print(x.shape)\n",
    "#lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "lstm_out = LSTM(64,activation='relu',return_sequences=True)(x)\n",
    "lstm_out = LSTM(32,activation='relu',return_sequences=True)(lstm_out )\n",
    "lstm_out = LSTM(16,activation='relu')(lstm_out )\n",
    "print(lstm_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION PART STARTS HERE\n",
    "attention_probs = Dense(16, activation='sigmoid', name='attention_vec')(lstm_out)\n",
    "#attention_mul=layers.merge([stm_out,attention_probs], output_shape],mode='concat',concat_axis=1))\n",
    "attention_mul =Multiply()([lstm_out, attention_probs])\n",
    "#attention_mul = merge([lstm_out, attention_probs],output_shape=32, name='attention_mul', mode='mul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 64, 39)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 62, 32)       3776        input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_103 (MaxPooling1D (None, 20, 32)       0           conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_180 (Dropout)           (None, 20, 32)       0           max_pooling1d_103[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                  (None, 20, 64)       24832       dropout_180[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                  (None, 20, 32)       12416       lstm_81[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_83 (LSTM)                  (None, 16)           3136        lstm_82[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 16)           272         lstm_83[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 16)           0           lstm_83[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_173 (Dense)               (None, 64)           1088        multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_181 (Dropout)           (None, 64)           0           dense_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_174 (Dense)               (None, 2)            130         dropout_181[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 45,650\n",
      "Trainable params: 45,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "output = Dense(64, activation='sigmoid')(attention_mul)\n",
    "output =Dropout(0.2)(output)\n",
    "output = Dense(2, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=1)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "138/138 [==============================] - 5s 33ms/step - loss: nan - accuracy: 0.4645 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 2/200\n",
      "138/138 [==============================] - 4s 26ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 3/200\n",
      "138/138 [==============================] - 4s 27ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 4/200\n",
      "138/138 [==============================] - 4s 28ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 5/200\n",
      "138/138 [==============================] - 4s 27ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 6/200\n",
      "138/138 [==============================] - 4s 27ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 7/200\n",
      "138/138 [==============================] - 4s 28ms/step - loss: nan - accuracy: 0.4651 - val_loss: nan - val_accuracy: 0.4784\n",
      "Epoch 8/200\n",
      "  6/138 [>.............................] - ETA: 2s - loss: nan - accuracy: 0.4115"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-297-c00b48d48174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\myproject\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_x,train_y, validation_data=(validation_x, validation_y),batch_size=64,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
